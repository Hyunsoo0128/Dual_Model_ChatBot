#!/usr/bin/env python3
"""
Amazon Bedrock Nova ë“€ì–¼ ëª¨ë¸ ì±—ë´‡
Nova Micro: ì¦‰ê°ì ì¸ ì´ˆê¸° ì‘ë‹µ
Nova Pro: ìƒì„¸í•œ ìµœì¢… ë‹µë³€

GitHub: https://github.com/your-username/amazon-nova-dual-chatbot
"""

import boto3
import json
import sys
import time
from concurrent.futures import ThreadPoolExecutor
from botocore.config import Config

class NovaDualChatbot:
    def __init__(self, region='us-east-1'):
        """
        Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        Args:
            region (str): AWS ë¦¬ì „ (ê¸°ë³¸ê°’: us-east-1)
        """
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ
        config = Config(
            read_timeout=60,
            connect_timeout=10,
            retries={'max_attempts': 3}
        )
        
        try:
            self.bedrock_runtime = boto3.client(
                service_name='bedrock-runtime',
                region_name=region,
                config=config
            )
            print(f"âœ… Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ë¦¬ì „: {region})")
        except Exception as e:
            print(f"âŒ Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ AWS ìê²© ì¦ëª…ê³¼ ë¦¬ì „ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            sys.exit(1)
    
    def invoke_nova_micro(self, prompt: str) -> str:
        """
        Nova Micro ëª¨ë¸ í˜¸ì¶œ - ì¦‰ê°ì ì¸ ì´ˆê¸° ì‘ë‹µ
        
        Args:
            prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: Nova Microì˜ ì‘ë‹µ
        """
        model_id = 'amazon.nova-micro-v1:0'
        
        # Nova Microìš© ìš”ì²­ ë³¸ë¬¸
        body = {
            "messages": [
                {
                    "role": "user",
                    "content": [{"text": prompt}]
                }
            ],
            "inferenceConfig": {
                "max_new_tokens": 100,
                "temperature": 0.3
            }
        }

        try:
            print("ğŸƒâ€â™‚ï¸ Nova Micro í˜¸ì¶œ ì¤‘...")
            start_time = time.time()
            
            response = self.bedrock_runtime.invoke_model(
                body=json.dumps(body),
                modelId=model_id,
                accept='application/json',
                contentType='application/json'
            )
            
            # ì‘ë‹µ íŒŒì‹±
            response_body = json.loads(response.get('body').read())
            completion = response_body['output']['message']['content'][0]['text']
            
            end_time = time.time()
            print(f"âš¡ Nova Micro ì‘ë‹µ ì™„ë£Œ ({end_time - start_time:.2f}ì´ˆ)")
            
            return completion.strip()
            
        except Exception as e:
            print(f"âŒ Nova Micro í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì´ˆê¸° ì‘ë‹µ ìƒì„±ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def stream_nova_pro(self, prompt: str):
        """
        Nova Pro ëª¨ë¸ ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ - ìƒì„¸í•œ ìµœì¢… ë‹µë³€
        
        Args:
            prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸
            
        Yields:
            str: Nova Proì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬
        """
        model_id = 'amazon.nova-pro-v1:0'
        
        # Nova Proìš© ìš”ì²­ ë³¸ë¬¸
        body = {
            "messages": [
                {
                    "role": "user", 
                    "content": [{"text": prompt}]
                }
            ],
            "inferenceConfig": {
                "max_new_tokens": 2048,
                "temperature": 0.5
            }
        }

        try:
            print("ğŸƒâ€â™€ï¸ Nova Pro ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...")
            
            response = self.bedrock_runtime.invoke_model_with_response_stream(
                body=json.dumps(body),
                modelId=model_id,
                accept='application/json',
                contentType='application/json'
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            stream = response.get('body')
            if stream:
                for event in stream:
                    chunk = event.get('chunk')
                    if chunk:
                        chunk_obj = json.loads(chunk.get('bytes').decode())
                        
                        # contentBlockDeltaì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        if 'contentBlockDelta' in chunk_obj:
                            delta = chunk_obj['contentBlockDelta'].get('delta', {})
                            text_content = delta.get('text', '')
                            if text_content:
                                yield text_content

        except Exception as e:
            print(f"âŒ Nova Pro ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            yield "\nì£„ì†¡í•©ë‹ˆë‹¤. ìƒì„¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def create_prompts(self, user_query: str):
        """
        ê° ëª¨ë¸ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            user_query (str): ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            tuple: (micro_prompt, pro_prompt)
        """
        micro_prompt = f"""ì‚¬ìš©ìê°€ ë‹¤ìŒ ì§ˆë¬¸ì„ í–ˆìŠµë‹ˆë‹¤: "{user_query}"

ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì´í•´í–ˆìœ¼ë©°, ìƒì„¸í•œ ë‹µë³€ì„ ì¤€ë¹„í•˜ê³  ìˆë‹¤ëŠ” ë‚´ìš©ì˜ ì§§ê³  ê²©ë ¤í•˜ëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""

        pro_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ìƒì„¸í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. 
ë³µì¡í•œ ì£¼ì œëŠ” ì´í•´í•˜ê¸° ì‰½ê²Œ ë‚˜ëˆ„ì–´ ì„¤ëª…í•˜ê³ , í•„ìš”í•œ ê²½ìš° ë§ˆí¬ë‹¤ìš´ì„ ì‚¬ìš©í•˜ì—¬ ì„œì‹ì„ ì§€ì •í•´ì£¼ì„¸ìš”.
í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ì§ˆë¬¸: "{user_query}" """

        return micro_prompt, pro_prompt

    def chat(self, user_query: str):
        """
        ë“€ì–¼ ëª¨ë¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë©”ì¸ í•¨ìˆ˜
        
        Args:
            user_query (str): ì‚¬ìš©ì ì§ˆë¬¸
        """
        print(f"\nğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸: {user_query}")
        print("=" * 60)
        
        # ê° ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
        micro_prompt, pro_prompt = self.create_prompts(user_query)
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=2) as executor:
            # ë‘ ì‘ì—…ì„ ë™ì‹œì— ì‹œì‘
            future_micro = executor.submit(self.invoke_nova_micro, micro_prompt)
            future_pro_stream = executor.submit(self.stream_nova_pro, pro_prompt)

            # Nova Micro ì‘ë‹µ ë¨¼ì € ì¶œë ¥
            micro_response = future_micro.result()
            print(f"\nğŸš€ [ì´ˆê¸° ì‘ë‹µ - Nova Micro]")
            print(f"ğŸ’­ {micro_response}")
            
            # Nova Pro ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì¶œë ¥
            print(f"\nğŸ“š [ìƒì„¸ ë‹µë³€ - Nova Pro]")
            pro_stream_generator = future_pro_stream.result()
            
            full_pro_response = ""
            for chunk in pro_stream_generator:
                # ì‹¤ì‹œê°„ íƒ€ì´í•‘ íš¨ê³¼
                sys.stdout.write(chunk)
                sys.stdout.flush()
                full_pro_response += chunk
                
            print(f"\n\nâœ… ì‘ë‹µ ì™„ë£Œ!")
            print("=" * 60)

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸ¤– Amazon Nova ë“€ì–¼ ëª¨ë¸ ì±—ë´‡")
    print("Nova Micro + Nova Pro ì¡°í•©ìœ¼ë¡œ ë¹ ë¥´ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
    print("=" * 60)
    
    # ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    try:
        chatbot = NovaDualChatbot()
    except Exception as e:
        print(f"âŒ ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_queries = [
        "AWS Lambdaì™€ EC2ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì˜¤ë²„í”¼íŒ…ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "Pythonì˜ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€?",
        "í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì˜ ì¥ì ì„ ì•Œë ¤ì£¼ì„¸ìš”."
    ]
    
    # ëŒ€í™”í˜• ëª¨ë“œ ë˜ëŠ” í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„ íƒ
    print("ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1: ëŒ€í™”í˜• ëª¨ë“œ (ì§ì ‘ ì§ˆë¬¸ ì…ë ¥)")
    print("2: í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ë¯¸ë¦¬ ì¤€ë¹„ëœ ì§ˆë¬¸)")
    
    mode = input("ì„ íƒ (1 ë˜ëŠ” 2): ").strip()
    
    if mode == "1":
        # ëŒ€í™”í˜• ëª¨ë“œ
        print("\nğŸ’¡ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥):")
        while True:
            user_input = input("\nğŸ™‹â€â™‚ï¸ ì§ˆë¬¸: ").strip()
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            if user_input:
                chatbot.chat(user_input)
    
    elif mode == "2":
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {len(test_queries)}ê°œ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
        for i, query in enumerate(test_queries, 1):
            print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}/{len(test_queries)}")
            chatbot.chat(query)
            
            if i < len(test_queries):
                input("\nâ¸ï¸  ë‹¤ìŒ í…ŒìŠ¤íŠ¸ë¡œ ì§„í–‰í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
    
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
